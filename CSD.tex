\documentclass{article}

\title{
	Using Constraints to Discover Sparse and Alternative Subgroup Descriptions
}
\author{
	Jakob Bach~\orcidlink{0000-0003-0301-2798}\\
	\small Karlsruhe Institute of Technology (KIT), Germany\\
	\small \href{mailto:jakob.bach@student.kit.edu}{jakob.bach@student.kit.edu}
}
\date{} % don't display a date

\usepackage[style=numeric, backend=bibtex]{biblatex}
\usepackage[ruled,linesnumbered,vlined]{algorithm2e} % pseudo-code
\usepackage{amsmath} % mathematical symbols
\usepackage{amssymb} % mathematical symbols
\usepackage{amsthm} % theorems, definitions etc.
\usepackage{booktabs} % nicely formatted tables (with top, mid, and bottom rule)
\usepackage{enumitem} % customized enumerations
\usepackage{graphicx} % plots
\usepackage{multirow} % cells spanning multiple rows in tables
\usepackage{orcidlink} % ORCID icon
\usepackage{subcaption} % figures with multiple sub-figures and sub-captions
\usepackage{hyperref} % links and URLs

\addbibresource{references.bib}

\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\begin{document}

\maketitle

\begin{abstract}
Subgroup-discovery methods allow users to obtain simple descriptions of interesting regions in a dataset.
However, existing methods have at most limited support for constraints, though the latter could improve interpretability even further.
In this article, we focus on two types of constraints:
First, we limit the number of features used in subgroups, making the subgroups sparse.
Second, we propose a novel optimization problem of finding alternative subgroup descriptions, which cover similar data objects as a given subgroup but use different features.
For both constraint types, we describe how to integrate them into heuristic search methods for subgroups.
Further, we propose a novel formulation of subgroup discovery as a white-box optimization problem, which allows solver-based search and is open for a variety of constraint types.
Additionally, we show that both constraint types lead to an $\mathcal{NP}$-hard optimization problem.
Finally, we employ 27 binary-classification datasets to compare heuristic and solver-based search for unconstrained and constrained subgroup discovery.
We observe that heuristic search methods often yield subgroups of high quality within a short runtime, also in scenarios with constraints.
\end{abstract}
%
\textbf{Keywords:} subgroup discovery, alternatives, constraints, satisfiability modulo theories, explainability, interpretability, XAI

\section{Introduction}
\label{sec:csd:introduction}

\paragraph{Motivation}

Interpretability of prediction models has significantly gained importance in the last few years~\cite{carvalho2019machine, molnar2020interpretable}.
There are various ways to foster interpretability in machine-learning pipelines.
In particular, some machine-learning models are simple enough to be intrinsically interpretable~\cite{carvalho2019machine}.
Subgroup-discovery methods fall into this category~\cite{atzmueller2015subgroup}.
The goal of subgroup discovery is to find `interesting' subgroups, i.e., subsets of the dataset, e.g., data objects where the prediction target takes a particular value.
Further, such subgroups should be described with a combination of simple conditions on feature values.
While these subgroup descriptions may already be understandable for users, we see further potential to increase interpretability with the help of constraints.

\paragraph{Problem statement}

This article addresses the problem of constrained subgroup discovery.
In particular, we focus on two types of constraints:

First, \emph{feature-cardinality constraints} limit the number of features used in subgroup descriptions.
Thus, the subgroup descriptions become \emph{sparse}, which increases their interpretability.
In particular, even intrinsically interpretable model may loose interpretability if they involve too many features~\cite{molnar2020interpretable}.
This constraint type resembles feature selection~\cite{guyon2003introduction, li2017feature}, which is a common task in machine-learning pipelines.

Second, we formulate constraints to search \emph{alternative subgroup descriptions}:
Given an \emph{original} subgroup, we search for further subgroups that reproduce the set of covered data objects as closely as possible but use different features in their subgroup description.
Thus, users obtain different explanations for the same subgroup.
Such alternative explanations are also popular in other explainable-AI techniques like counterfactuals~\cite{mothilal2020explaining, russell2019efficient}, e.g., to enable users to develop and test multiple hypotheses or foster trust in the predictions~\cite{kim2021multi, wang2019designing}.

\paragraph{Related work}

There are various search methods for subgroup discovery, exact~\cite{bosc2018anytime, lemmerich2010fast, millot2020optimal} as well as heuristic~\cite{friedman1999bump, leeuwen2012diverse, mampaey2012efficient, proencca2022robust} ones.
We see research gaps in three aspects:
First, if these methods support constraints at all, then typically only a limited set of constraints, as the search routines need to be specifically adapted to particular constraint types.
Second, the number of features used in a subgroup is a well-known measure for subgroup complexity~\cite{helal2016subgroup, herrera2011overview, ventura2018subgroup}.
However, there is no systematic evaluation of this constraint type, particularly regarding different cardinality bounds and comparing multiple subgroup-discovery methods.
Third, various subgroup-discovery methods yield a diverse set of subgroups rather than only one subgroup, thereby providing alternative solutions~\cite{bosc2018anytime, lemmerich2010fast, leeuwen2012diverse, proencca2022robust}.
However, these alternatives target at covering different subsets of data objects from the dataset.
In contrast, our notion of alternative subgroup descriptions tries to capture the same data objects as in the original subgroup but with different features.

\paragraph{Contributions}

Our contribution is fivefold:

First, we formalize subgroup discovery as a white-box optimization problem.
This formulation admits solver-based search for subgroups and allows integrating a broad range of constraints.

Second, we formalize two constraint types, i.e., feature-cardinality constraints and alternative subgroup descriptions.
For the latter, we allow users to control alternatives with two parameters, i.e., the number of alternatives and a dissimilarity threshold.
Both constraint types can be combined with our white-box formulation of subgroup discovery.

Third, we describe how to integrate these two constraint types into three existing heuristic search methods and two novel baselines for subgroup discovery.

Fourth, we analyze the computational complexity of the subgroup-discovery problem with each of these two constraint types.
In particular, we prove several $\mathcal{NP}$-hardness results.

Fifth, we conduct comprehensive experiments with 27 binary-classification datasets from the Penn Machine Learning Benchmarks (PMLB)~\cite{olson2017pmlb, romano2021pmlb}.
We evaluate the runtime of subgroup discovery and the quality of the discovered subgroups.
In particular, we compare solver-based and heuristic subgroup-discovery methods in three experimental scenarios:
without constraints, with feature-cardinality constraints, and searching alternative subgroup descriptions.
For the latter, we vary the two user parameters for alternatives.
In a fourth experimental scenario, we analyze how the subgroup quality in solver-based search for alternatives depends on the timeout of the solver.
We publish all code\footnote{\url{https://github.com/Jakob-Bach/Constrained-Subgroup-Discovery}} and experimental data\footnote{TODO} online.

\paragraph{Experimental results}

In our experimental scenario without constraints, the heuristic search methods yield similar subgroup quality as solver-based search.
On the test set, the heuristics may even be better since they show less overfitting, i.e., a lower gap between training-set quality and test-set quality.
Additionally, the solver-based search is considerably slower.
Using a solver timeout, a large fraction of the final subgroup quality can be reached in a fraction of the runtime, though this quality is lower than for equally fast heuristics.

With feature-cardinality constraints, heuristic search methods still are competitive quality-wise compared to solver-based search.
Further, subgroups only using a few features show relatively high quality compared to unconstrained subgroups.
I.e., there is a decreasing marginal utility in selecting more features.
Additionally, feature-cardinality constraints reduce overfitting.

For alternative subgroup description, heuristics also yield similar quality as solver-based search.
Our two user parameters for alternatives control the solutions as expected:
The similarity to the original subgroup and the quality of the alternatives decrease for more alternatives and a higher dissimilarity treshold.

\paragraph{Outline}

Section~\ref{sec:csd:fundamentals} introduces notation and fundamentals.
Section~\ref{sec:csd:approach} describes and analyzes constrained subgroup discovery.
Section~\ref{sec:csd:related-work} reviews related work.
Section~\ref{sec:csd:experimental-design} outlines our experimental design, while Section~\ref{sec:csd:evaluation} presents the experimental results.
Section~\ref{sec:csd:conclusion} concludes and discusses future work.
Appendix~\ref{sec:csd:appendix} contains supplementary materials.

\section{Fundamentals}
\label{sec:csd:fundamentals}

In this section, we describe fundamentals for our work.
First, we introduce notation (cf.~Section~\ref{sec:csd:fundamentals:notation}).
Next, we describe the problem of subgroup discovery (cf.~Section~\ref{sec:csd:fundamentals:subgroup-discovery}) and common heuristic search methods to solve this problem (cf.~Section~\ref{sec:csd:fundamentals:heuristics}).
Finally, we propose two baselines for subgroup discovery that we also use in our experiments. (cf.~Section~\ref{sec:csd:fundamentals:baselines}).

\subsection{Notation}
\label{sec:csd:fundamentals:notation}

$X \in \mathbb{R}^{m \times n}$ stands for a dataset in the form of a matrix.
Each row is a data object, and each column is a feature.
$F = \{f_1, \dots, f_n\}$ is the corresponding set of feature names.
We assume that categorical features have already been made numeric, e.g., via one-hot or ordinal encoding.
There are also subgroup-discovery methods that focus on categorical data.
$X_{\cdot{}j} \in \mathbb{R}^m$ denotes the vector representation of the $j$-th feature.
$y \in Y^m$ represents the prediction target with domain $Y$, e.g., $Y=\{0,1\}$ for binary classification or $Y=\mathbb{R}$ for regression.
In this paper, we focus on binary-classification tasks in our formalization and evaluation.
In principle, subgroup discovery also works for regression problems.

$\mathit{lb}, \mathit{ub}$
In subgroup discovery, one ... (goal, decision variables)
--either definition or equation
-- first general (interesting region, then for binary classification)

\begin{definition}[Subgroup]
	\emph{subgroup}
	\label{def:csd:subgroup}
\end{definition}

\begin{definition}[Subgroup discovery]
	\emph{subgroup discovery}
	\label{def:csd:subgroup-discovery}
\end{definition}

Without loss of generality, we assume that the class with label~`1' is the class of interest, also called \emph{positive} class, which should be captured by subgroups.
The function $Q(\mathit{lb}, \mathit{ub}, X, y)$ returns the quality of such a subgroup.
Without loss of generality, we assume that this function should be maximized.

\subsection{Subgroup Discovery}
\label{sec:csd:fundamentals:subgroup-discovery}

\cite{helal2016subgroup} \cite{herrera2011overview} \cite{atzmueller2015subgroup} \cite{ventura2018subgroup} \cite{meeng2021real}

\paragraph{Problem}

subgroup == (hyper)box

introduce (here or in notation) the Iverson bracket [], which turns Boolean expressions into 0/1

\paragraph{Evaluation}

A popular evaluation metric for subgroup quality is Weighted Relative Accuracy (WRAcc)~\cite{lavravc1999rule}:
%
\begin{equation}
	\text{WRACC} = \frac{m_b}{m} \cdot \left( \frac{m_b^+}{m_b} - \frac{m^+}{m} \right)
	\label{eq:csd:wracc}
\end{equation}
%
Besides the total number of data objects~$m$, this metric considers the number of positive data objects~$m^+$, the number of data objects in the subgroup~$m_b$, and the number of positive data objects in the subgroup~$m_b^+$.
In particular, WRAcc is the product of two factors:
$m_b / m$ denotes the relative frequency of subgroup membership, i.e., the generality of the subgroup.
The second factor measures the difference in relative frequency of the positive class between the subgroup and the whole dataset, i.e., the relative accuracy of the subgroup.
If the subgroup contains the same fraction of positive data objects as the whole dataset, WRAcc is zero.
The maximum and minimum of WRAcc depend on the class frequencies.
In particular, the maximum WRAcc equals the product of the relative frequencies of positive and negative data objects in the dataset:
%
\begin{equation}
	\text{WRACC}_{\text{max}} = \frac{m^+}{m} \cdot \left( 1 - \frac{m^+}{m} \right)
	\label{eq:csd:wracc-max}
\end{equation}
%
This maximum is reached if all positive data objects are in the subgroup and all negative data objects are outside, i.e., $m_b^+ = m_b = m^+$.
While the maximum is 0.25 if both classes occur with equal frequency, it becomes smaller the more imbalanced the classes are.
Thus, it makes sense to normalize WRAcc when working with datasets with different relative class frequencies.
One normalization, which we use in our experiments, is a max-normalization to the range $[-1, 1]$~\cite{mathonat2021anytime}:
%
\begin{equation}
	\text{nWRACC} = \frac{\text{WRACC}}{\text{WRACC}_{\text{max}}} = \frac{m_b^+ \cdot m - m^+ \cdot m_b}{m^+ \cdot (m - m^+)}
	\label{eq:csd:wracc-normalized}
\end{equation}
%
Alternatively, one can also min-max normalize the range to~$[0, 1]$~\cite{carmona2018unifying, ventura2018subgroup}.

\subsection{Heuristic Search Methods for Subgroup Discovery}
\label{sec:csd:fundamentals:heuristics}

In this section, we discuss three heuristic search methods for subgroup discovery.
All of them are well-established and originate from related work.

\begin{algorithm}[t]
	\DontPrintSemicolon
	\KwIn{Dataset~$X \in \mathbb{R}^{m \times n}$, \newline
		Prediction target~$y \in \{0, 1\}^n$, \newline
		Subgroup-quality function~$Q(\mathit{lb}, \mathit{ub}, X, y)$, \newline
		Peeling fraction~$\alpha \in (0, 1)$, \newline
		Support threshold~$\beta_0 \in [0, 1]$
	}
	\KwOut{Subgroup bounds~$\mathit{lb}, \mathit{ub} \in \mathbb{R}^n$}
	\BlankLine
	\caption{\emph{PRIM} for subgroup discovery.}
	\label{al:csd:prim}
\end{algorithm}

\paragraph{PRIM}

\emph{Patient Rule Induction Method (PRIM)}~\cite{friedman1999bump}

Algorithm~\ref{al:csd:prim}

according to Vadim's Medium blog post, REDS article should say that pasting phase of PRIM has little effect on quality

Idea: in pseudocode, already use function getPermissibleFeatureIdxs(), so that part can be plugged in later (when introducing cardinality constraint) without having to repeat whole algorithm

\begin{algorithm}[t]
	\DontPrintSemicolon
	\KwIn{Dataset~$X \in \mathbb{R}^{m \times n}$, \newline
		Prediction target~$y \in \{0, 1\}^n$, \newline
		Subgroup-quality function~$Q(\mathit{lb}, \mathit{ub}, X, y)$, \newline
		Beam width~$w \in \mathbb{N}$
	}
	\KwOut{Subgroup bounds~$\mathit{lb}, \mathit{ub} \in \mathbb{R}^n$}
	\BlankLine
	\caption{\emph{Beam Search} for subgroup discovery.}
	\label{al:csd:beam-search}
\end{algorithm}

\paragraph{Beam search}

Algorithm~\ref{al:csd:beam-search}

\begin{algorithm}[t]
	\DontPrintSemicolon
	\KwIn{Dataset~$X \in \mathbb{R}^{m \times n}$, \newline
		Prediction target~$y \in \{0, 1\}^n$, \newline
		Subgroup-quality function~$Q(\mathit{lb}, \mathit{ub}, X, y)$, \newline
		Beam width~$w \in \mathbb{N}$
	}
	\KwOut{Subgroup bounds~$\mathit{lb}, \mathit{ub} \in \mathbb{R}^n$}
	\BlankLine
	\caption{\emph{Best Interval} for subgroup discovery.}
	\label{al:csd:best-interval}
\end{algorithm}

\paragraph{Best interval}

\emph{Best Interval}~\cite{mampaey2012efficient}

Algorithm~\ref{al:csd:best-interval}

\subsection{Baselines for Subgroup Discovery}
\label{sec:csd:fundamentals:baselines}

In this section, we propose two baselines for subgroup discovery, \emph{MORB} (cf.~Section~\ref{sec:csd:fundamentals:baselines:morb}) and \emph{Random Search} (cf.~Section~\ref{sec:csd:fundamentals:baselines:random-search}).
They are conceptually simpler than the heuristic search methods (cf.~Section~\ref{sec:csd:fundamentals:heuristics}) and serve as further reference points in our experiments.
While they technically also are heuristics, we use the term \emph{baselines} to refer to these two methods specifically.

\begin{algorithm}[t]
	\DontPrintSemicolon
	\KwIn{Dataset~$X \in \mathbb{R}^{m \times n}$, \newline
		Prediction target~$y \in \{0, 1\}^n$
	}
	\KwOut{Subgroup bounds~$\mathit{lb}, \mathit{ub} \in \mathbb{R}^n$}
	\BlankLine
	\For{$j \leftarrow 1$ \KwTo $n$}{
		$\mathit{lb}_j \leftarrow \min\limits_{\substack{i \in \{1, \dots, m\} \\ y_i = 1}} X_{ij}$\;
		\lIf{$\mathit{lb}_j = \min_{i \in \{1, \dots, m\}} X_{ij}$}{$\mathit{lb}_j \leftarrow -\infty$}
		$\mathit{ub}_j \leftarrow \max\limits_{\substack{i \in \{1, \dots, m\} \\ y_i = 1}} X_{ij}$\;
		\lIf{$\mathit{ub}_j = \max_{i \in \{1, \dots, m\}} X_{ij}$}{$\mathit{ub}_j \leftarrow \infty$}
	}
	\Return{$\mathit{lb}, \mathit{ub}$}
	\caption{\emph{MORB} for subgroup discovery.}
	\label{al:csd:morb}
\end{algorithm}

\subsubsection{MORB}
\label{sec:csd:fundamentals:baselines:morb}

This baseline builds on the following definition:
%
\begin{definition}[Minimal Optimal Recall Box (MORB)]
	The \emph{Minimal Optimal Recall Box (MORB)} is the subgroup (cf.~Definition~\ref{def:csd:subgroup}) whose lower/upper bounds of each feature correspond to the minimum/maximum value of that feature for all positive data objects in the dataset.
	\label{def:csd:morb}
\end{definition}
%
This criterion ensures that all positive data objects are contained in the subgroup.
Thus, the evaluation metric `recall', i.e., fraction of positive data objects in the subgroup, is~1, i.e., optimal.
At the same time, moving the lower bounds higher or moving the upper bounds lower would exclude positive data objects from the subgroup.
In this sense, the bounds are minimal.
The corresponding subgroup is unique.

Algorithm~\ref{al:csd:morb} outlines the procedure to determine these bounds.
Slightly deviating from the previously described criterion, we set bounds to infinity if the minimum/maximum feature value of positive data objects equals the minimum/maximum feature value of all data objects.
I.e., if a bound does not exclude any negative data object from the subgroup on the given data, we ensure that this property also holds for any new data, where global feature minima/maxima may differ.

Since \emph{MORB} only needs to iterate over all data objects and features once to determine the minima and maxima, the computational complexity of this heuristic is~$O(n \cdot m)$.
Further, the heuristic is not only fast, but also optimal in certain scenarios.
To describe these scenarios, we introduce the following term:
%
\begin{definition}[Perfect subgroup]
	Given a dataset~$X \in \in \mathbb{R}^{m \times n}$ with prediction target~$y \in \{0, 1\}^n$,
	a \emph{perfect subgroup} is a subgroup (cf.~Definition~\ref{def:csd:subgroup}) that contains all positive data objects (i.e., with $y_i = 1$) but zero negative data objects (i.e., with $y_i = 0$) from the dataset.
	\label{def:csd:perfect-subgroup}
\end{definition}
%
Next, we define a corresponding optimization problem:
%
\begin{definition}[Perfect-subgroup discovery]
	\emph{Perfect-subgroup discovery} is the problem of finding a perfect subgroup (cf.~Definition~\ref{def:csd:perfect-subgroup}) if it exists or determining that it does not exist.
	\label{def:csd:perfect-subgroup-discovery}
\end{definition}
%
\emph{MORB} solves this problem in~$O(n \cdot m)$.
In particular, after \emph{MORB} has found a subgroup, we only need to check whether the subgroup contains any negative data objects.
If the found subgroup does not contain negative data objects, then it is perfect.
If it does, then no perfect subgroup exists.
In particular, the bounds found by \emph{MORB} cannot be made tighter to exclude negative data objects from the subgroup without also excluding positive data objects, thereby violating perfection.

Having \emph{MORB} as an efficient algorithm for perfect-subgroup discovery, we obtain the following complexity result:
%
\begin{proposition}[Complexity of perfect-subgroup discovery]
	Perfect-subgroup discovery (cf.~Definition~\ref{def:csd:perfect-subgroup-discovery}) can be solved in polynomial runtime regarding the dataset size~$m \cdot n$.
	\label{prop:csd:complexity-unconstrained-perfect-subgroup}
\end{proposition}

\begin{algorithm}[t]
	\DontPrintSemicolon
	\KwIn{Dataset~$X \in \mathbb{R}^{m \times n}$, \newline
		Prediction target~$y \in \{0, 1\}^n$, \newline
		Subgroup-quality function~$Q(\mathit{lb}, \mathit{ub}, X, y)$, \newline
		Number of iterations~$n\_iters \in \mathbb{N}$
	}
	\KwOut{Subgroup bounds~$\mathit{lb}, \mathit{ub} \in \mathbb{R}^n$}
	\BlankLine
	\caption{\emph{Random Search} for subgroup discovery.}
	\label{al:csd:random-search}
\end{algorithm}

\subsubsection{Random Search}
\label{sec:csd:fundamentals:baselines:random-search}

Algorithm~\ref{al:csd:random-search}

\section{Constrained Subgroup Discovery}
\label{sec:csd:approach}

In this section, we discuss subgroup discovery with constraints.
First, we frame subgroup discovery as an SMT optimization problem (cf.~Section~\ref{sec:csd:approach:smt}).
Second, we give a brief overview of potential constraint types (cf.~Section~\ref{sec:csd:approach:constraint-types}).
Third, we formalize and analyze feature-cardinality constraints (cf.~Section~\ref{sec:csd:approach:cardinality}).
Fourth, we formalize and analyze alternative subgroup descriptions (cf.~Section~\ref{sec:csd:approach:alternatives}).

\subsection{SMT Encoding of Subgroup Discovery}
\label{sec:csd:approach:smt}

To find optimal subgroups exactly, one can encode subgroup discovery as a white-box optimization problem and employ a solver.
Here, we propose a Satisfiability Module Theories (SMT)~\cite{barrett2018satisfiability} encoding, which is straightforward given the problem definition (cf.~Section~\ref{sec:csd:fundamentals:subgroup-discovery}).
SMT generally allows expressions in first-order logic with particular interpretations, e.g., arrays, arithmetic, or bit vectors~\cite{barrett2018satisfiability}.
For our encoding of optimal subgroup discovery, we use linear real arithmetic (LRA).
Complementing this SMT encoding, Appendix~\ref{sec:csd:appendix:further-encodings} describes further encodings:
categorical variables in SMT (cf.~Section~\ref{sec:csd:appendix:further-encodings:smt-categorical}), mixed-integer linear programming (cf.~Section~\ref{sec:csd:appendix:further-encodings:milp}) and maximum satisfiability (cf.~Section~\ref{sec:csd:appendix:further-encodings:max-sat}).

The optimization problems consists of an objective function and constraints, which we both describe in the following.

\paragraph{Objective function}

As objective function, we use WRAcc, which should be maximized.
In the formula for WRACC (cf.~Equation~\ref{eq:csd:wracc}), $m$ and $m^+$ are constants, while $m_b$ and $m_b^+$ depend on the decision variables.
The previously provided formula seems to be non-linear in the decision variables since $m_b$ appears in enumerator and denominator.
However, one can reformulate the expression by multiplying its two factors, obtaining the following expression:
%
\begin{equation}
	\text{WRACC} = \frac{m_b^+}{m} - \frac{m_b \cdot m^+}{m^2} = \frac{m_b^+ \cdot m - m_b \cdot m^+}{m^2}
	\label{eq:csd:smt-wracc}
\end{equation}
%
In this new expression, the denominators are constant and the factor~$m^+$ in the enumerator is constant as well.
Thus, the whole expression is linear in~$m_b^+$ and~$m_b$.
We define these two quantities as linear expressions from binary decision variables~$b \in \{0, 1\}^m$ that denote \emph{subgroup membership}.
I.e., $b_i$~expresses whether the $i$-th data object is in the subgroup or not:
%
\begin{equation}
	\begin{aligned}
		 m_b &:= \sum_{i=1}^{m} b_i \\
		 m_b^+ &:= \sum_{\substack{i \in \{1, \dots, m\} \\ y_i = 1 }} b_i \\
	\end{aligned}
	\label{eq:csd:smt-constraint-m-as-sum}
\end{equation}
%
Since one know the values of the target variable~$y$ a priori, one only needs to sum over the affected, i.e.. positive, data objects in the expression for~$m_b^+$.
Further, defining $m_b^+$ and~$m_b$ as separate integer variables is optional.
One can also directly insert their expressions into Equation~\ref{eq:csd:smt-wracc}.
We choose this formulation in our implementation and therefore wrote $:=$ instead of using a proper propositional operator like $\leftrightarrow$ in Equation~\ref{eq:csd:smt-constraint-m-as-sum}.

The formula for nWRAcc (cf.~Equation~\ref{eq:csd:wracc-normalized}) is linear as well, having the same enumerator as Equation~\ref{eq:csd:smt-wracc} and a different constant in the denominator.

\paragraph{Constraints}

The subgroup membership~$b_i$ of a data object depends on the bounds of the subgroup.
Thus, we define real-valued decision variables $\mathit{lb}, \mathit{ub} \in \mathbb{R}^n$ for the latter.
In particular, there is one lower bound and one upper bound for each of the $n$~features.
The upper bounds naturally need to be at least as high as the lower bounds:
%
\begin{equation}
	\forall j \in \{1, \dots, n\}:~ \mathit{lb}_j\leq \mathit{ub}_j
	\label{eq:csd:smt-constraint-bounds-monotonic}
\end{equation}
%
A data object is a member of the subgroup if all its feature values are contained within the bounds:
%
\begin{equation}
	\forall i \in \{1, \dots, m\}:~ b_i\leftrightarrow \bigwedge_{j \in \{1, \dots, n\}} \left( \left( X_{ij} \geq \mathit{lb}_j \right) \land \left( X_{ij} \leq \mathit{ub}_j \right) \right)
	\label{eq:csd:smt-constraint-subgroup-membership}
\end{equation}
%
Instead of defining separate decision variables~$b_i$ and binding them to the bounds with a equivalence constraint, one could also insert the Boolean expression on the right-hand-side into Equation~\ref{eq:csd:smt-constraint-m-as-sum} directly.
In particular, $\mathit{lb}_j$ and $\mathit{ub}_j$ are the only decision variables that are strictly necessary in the optimization problem.
However, for formulating some constraint types on subgroups (cf.~Section~\ref{sec:csd:approach:constraint-types}), it is helpful to be able to refer to~$b_i$.

\paragraph{Complete optimization problem}

Combining all prior definitions of decision variables, constraints, and the objective function, we obtain the following SMT optimization problem:

\begin{equation}
	\begin{aligned}
		\max &\quad & Q_{\text{WRAcc}} &= \frac{m_b^+}{m} - \frac{m_b \cdot m^+}{m^2} \\
		\text{s.t.:} &\quad & m_b &:= \sum_{i=1}^{m} b_i \\
		&\quad & m_b^+ &:= \sum_{\substack{i \in \{1, \dots, m\} \\ y_i = 1 }} b_i \\
		&\quad \forall i \in \{1, \dots, m\} & b_i &\leftrightarrow \bigwedge_{j \in \{1, \dots, n\}} \left( \left( X_{ij} \geq \mathit{lb}_j \right) \land \left( X_{ij} \leq \mathit{ub}_j \right) \right) \\
		&\quad \forall j \in \{1, \dots, n\} & \mathit{lb}_j &\leq \mathit{ub}_j \\
		&\quad & b &\in \{0, 1\}^m \\
		&\quad & \mathit{lb}, \mathit{ub} &\in \mathbb{R}^n
	\end{aligned}
	\label{eq:csd:smt-problem-unconstrained-complete}
\end{equation}

We refer to this optimization problem as \emph{unconstrained subgroup discovery} in the following since it only contains constraints that are technically necessary to define subgroup discovery properly but no additional user constraints.

\subsection{Overview of Constraint Types}
\label{sec:csd:approach:constraint-types}

domain-knowledge
secondary objectives
regularization
alternatives (covering different data objects and covering same data objects differently)

existing methods need adaptation to support certain constraint types, white-box formulation like SMT is more general

according to \cite{meeng2021real}, lower bounds on data objects and subgroup quality are common constraints in SD algorithms
\cite{lavravc2006relevancy} mentions feature-cardinality constraints, recall constraints
\cite{helal2016subgroup, herrera2011overview, ventura2018subgroup} mention number of features (and number os subgroups) as complexity measure
(non-user) quality constraints are also used within SD algorithms for refinement/pruning \cite{atzmueller2015subgroup, grosskreutz2009subgroup}

\subsection{Feature-Cardinality Constraints}
\label{sec:csd:approach:cardinality}

In this section, we discuss feature-cardinality constraints for subgroup discovery.
First, we motivate and formalize them (cf.~Section~\ref{sec:csd:approach:cardinality:concept}).
Next, we describe how to integrate them into our SMT encoding of subgroup discovery (cf.~Section~\ref{sec:csd:approach:cardinality:smt}), heuristic search methods (cf.~Section~\ref{sec:csd:approach:cardinality:heuristics}), and baselines (cf.~Section~\ref{sec:csd:approach:cardinality:baselines}).
Finally, we analyze the computational complexity of subgroup discovery with this constraint type (cf.~Section~\ref{sec:csd:approach:cardinality:complexity}).

\subsubsection{Concept}
\label{sec:csd:approach:cardinality:concept}

define term selected == restricted

note that only constraints added, rest of optimization problem (particularly objective) remains as-is

give reason why less than $k$ features may be selected

\begin{definition}[Feature-cardinality constraint]
	\emph{feature-cardinality constraint}
	\label{def:csd:feature-cardinality-constraint}
\end{definition}

\subsubsection{SMT Encoding}
\label{sec:csd:approach:cardinality:smt}

We first need to define whether a feature is selected or not.
Thus, we introduce binary decision variables $s, s^{\text{lb}}, s^{\text{ub}} \in \{0, 1\}^n$.
A feature is selected if its bounds exclude at least one data object from the subgroup, i.e., the lower bound is higher than the minimum feature value or the upper bound is lower than the maximum feature value:
%
\begin{equation}
	\begin{aligned}
		\forall j: & & s^{\text{lb}}_j &\leftrightarrow \left( \mathit{lb}_j > \min_{i \in \{1, \dots, m\}} X_{ij} \right) \\
		\forall j: & &s^{\text{ub}}_j &\leftrightarrow \left( \mathit{ub}_j < \max_{i \in \{1, \dots, m\}} X_{ij} \right) \\
		\forall j: & & s_j &\leftrightarrow \left( s^{\text{lb}}_j \lor s^{\text{ub}}_j \right) \\
		\text{with index:} & & j &\in \{1, \dots, n\}
	\end{aligned}
	\label{eq:csd:smt-constraint-feature-selection}
\end{equation}
%
Given the definition of~$s_j$, setting an upper bound on the number of selected features is straightforward:
%
\begin{equation}
	\sum_{j=1}^n s_j \leq k
	\label{eq:csd:smt-constraint-feature-cardinalty}
\end{equation}
%
Instead of defining decision variables $s_j$, $s^{\text{lb}}_j$, and $s^{\text{ub}}_j$, one could also insert the corresponding expressions into Equation~\ref{eq:csd:smt-constraint-feature-cardinalty} directly.
However, we will use~$s_j$ for alternative subgroup descriptions (cf.~Section~\ref{sec:csd:approach:alternatives:smt}) as well.

The overall optimization problem of subgroup discovery with feature-cardinality constraints is the one of unconstrained subgroup discovery (cf.~Equation~\ref{eq:csd:smt-problem-unconstrained-complete}) supplemented by the variables and constraints from Equations~\ref{eq:csd:smt-constraint-feature-selection} and~\ref{eq:csd:smt-constraint-feature-cardinalty}.

\subsubsection{Integration into Heuristic Search Methods}
\label{sec:csd:approach:cardinality:heuristics}

maybe mention antimonotonicity (see definition below)

\subsubsection{Integration into Baselines}
\label{sec:csd:approach:cardinality:baselines}

\subsubsection{Computational Complexity}
\label{sec:csd:approach:cardinality:complexity}

\paragraph{Hardness}

(cf.~Appendix~\ref{sec:csd:appendix:proofs:cardinality} for the proof)

\begin{proposition}[Complexity of]
	\label{prop:csd:complexity-cardinality-perfect-subgroup}
\end{proposition}

(cf.~Appendix~\ref{sec:csd:appendix:proofs:cardinality} for the proof)

\begin{proposition}[Complexity of]
	\label{prop:csd:complexity-cardinality-imperfect-subgroup}
\end{proposition}

\paragraph{Parameterized complexity}

\begin{proposition}[Parameterized complexity of]
	\label{prop:csd:complexity-unconstrained-xp}
\end{proposition}

\begin{proposition}[Parameterized complexity of]
	\label{prop:csd:complexity-cardinality-xp}
\end{proposition}

\subsection{Alternative Subgroup Descriptions}
\label{sec:csd:approach:alternatives}

In this section, we propose the optimization problem of alternative subgroup descriptions.
First, we motivate and formalize the problem (cf.~Section~\ref{sec:csd:approach:alternatives:concept}).
Next, we describe how to phrase it within our SMT encoding of subgroup discovery (cf.~Section~\ref{sec:csd:approach:alternatives:smt}) and heuristic search methods (cf.~Section~\ref{sec:csd:approach:alternatives:heuristics}).
Finally, we analyze the computational complexity of this problem (cf.~Section~\ref{sec:csd:approach:alternatives:complexity}).

\subsubsection{Concept}
\label{sec:csd:approach:alternatives:concept}

For alternative subgroup descriptions, we assume to have an \emph{original subgroup} given, with subgroup membership~$b^{(0)} \in \{0, 1\}^m$ of data objects and with feature selection~$s^{(0)} \in \{0, 1\}^n$.
When searching alternatives, we do not optimize subgroup quality but the similarity to the original subgroup. 
We express this similarity in terms of subgroup membership.
If this similarity is very high, then the subgroup quality should also be similar.

Also, we constrain the new subgroup descriptions to be alternative enough.
We express this dissimilarity in terms of the subgroups' feature selection.
The user chooses a dissimilarity threshold~$\tau \in \mathbb{R}_{\geq 0}$ and can thereby control alternatives.
We recommend to employ a feature-cardinality constraint as well, so there are sufficiently many features left for selection in the alternative descriptions.

In a nutshell, alternative subgroup descriptions should produce similar predictions as the original subgroup but use different features.

\paragraph{Sequential search}

One can search for multiple alternative subgroup descriptions sequentially.
After determining the original subgroup, each iteration yields one further alternative.
The user may prescribe a number of alternatives~$a \in \mathbb{N}$ a priori or interrupt the procedure whenever the alternatives are not interesting anymore, e.g., too dissimilar to the original subgroup.
Each alternative should have similar subgroup membership as the original subgroup but a dissimilar feature selection compared to all \emph{existing subgroups}, i.e., subgroups found in prior iterations.
The following definition captures this optimization problem:
%
\begin{definition}[Alternative-Subgroup-Description Discovery]
	Given
	\begin{itemize}[noitemsep]
		\item a dataset $X \in \mathbb{R}^{m \times n}$ with prediction target~$y \in \{0, 1\}^n$,
		\item $a-1 \in \mathbb{N}$ existing subgroups with subgroup membership~$b^{(l)} \in \{0, 1\}^m$ and feature selection~$s^{(l)} \in \{0, 1\}^n$ for $l \in \{0, \dots, a - 1\}$,
		\item a similarity measure $\text{sim}(\cdot)$ for subgroup-membership vectors,
		\item a dissimilarity measure $\text{dis}(\cdot)$ for feature-selection vectors of subgroups,
		\item and a dissimilarity threshold~$\tau \in \mathbb{R}_{\geq 0}$,
	\end{itemize}
	\emph{alternative-subgroup-description discovery} is the problem of finding a subgroup (cf.~Definition~\ref{def:csd:subgroup}) with membership~$b^{(a)} \in \{0, 1\}^m$ and feature selection~$s^{(a)}$ that maximizes the subgroup-membership similarity $\text{sim}(b^{(a)}, b^{(0)})$ to the original subgroup while being dissimilar to all existing subgroups regarding the feature selection, i.e. $\forall l \in \{0, \dots, a-1\}:~\text{dis}(s^{(a)}, s^{(l)}) \geq \tau$.
	\label{def:csd:alternative-subgroup-description-discovery}
\end{definition}
%
In the following, we discuss our choice of $\text{sim}(\cdot)$ and $\text{dis}(\cdot)$.

\paragraph{Similarity in objective function}

There are various options to quantify the similarity between subgroup-membership vectors.
For example, the Hamming distance counts how many vector entries differ~\cite{choi2010survey}.
We turn this distance into a similarity measure by counting identical vector entries.
Further, we normalize the similarity with the vector length, i.e., number of data objects~$m$, to obtain the following \emph{normalized Hamming similarity} for two subgroup-membership vectors~$b', b'' \in \{0, 1\}^m$:
%
\begin{equation}
	\text{sim}_{\text{nHamm}}(b', b'') = \frac{\sum_{i=1}^{m} [b'_i = b''_i]}{m}
	\label{eq:csd:hamming-general}
\end{equation}
%
If either $b'$ or $b''$ is constant, then this similarity measure is linear in its remaining argument.
Further, if one considers one vector to be a prediction and the other one to be the ground truth, Equation~\ref{eq:csd:hamming-general} equals prediction accuracy.

Another popular similarity measure for sets or binary vectors is the Jaccard index~\cite{choi2010survey}, which relates the overlap of positive vector entries to their union:
%
\begin{equation}
	\text{sim}_{\text{Jacc}}(b', b'') = \frac{\sum_{i=1}^{m} [b'_i \land b''_i ]}{\sum_{i=1}^{m} [b'_i \lor b''_i]}
	\label{eq:csd:jaccard}
\end{equation}
%
However, this similarity measure is not linear in~$b'$ and~$b''$, which prevents its use in certain white-box solvers.
Thus, we use the normalized Hamming similarity as the objective function.

\paragraph{Dissimilarity in constraints}

There are various options to quantify the dissimilarity between feature-selection vectors.
We employ the following \emph{deselection dissimilarity} in combination with an adapted dissimilarity threshold:
%
\begin{equation}
	\text{dis}_{\text{des}}(s^{\text{new}}, s^{\text{old}}) = \sum_{j=1}^n [\lnot s^{\text{new}}_j \land s^{\text{old}}_j] \geq \min \left( \tau_{\text{abs}},~k^{\text{old}} \right)
	\label{eq:csd:constraint-dissimilarity}
\end{equation}
%
This dissimilarity counts how many of the previously selected features are \emph{not} selected in the new subgroup description.
These features may either be replaced by other features or the total number of selected features may be reduced.
The constraints ensures that at least $\tau_{\text{abs}} \in \mathbb{N}$ features are deselected but never more than there were selected before ($k^{\text{old}}$), which would be infeasible.
For maximum dissimilarity, none of the previously selected features may be selected again.
Note that the dissimilarity measure is asymmetric, i.e., $\text{dis}_{\text{des}}(s^{\text{new}}, s^{\text{old}}) \neq \text{dis}_{\text{des}}(s^{\text{old}}, s^{\text{new}})$.
While this property would be an issue in a simultaneous search for multiple alternatives, i.e., without clear ordering, it is acceptable for sequential search, where `old' and `new' are well-defined.

Conceptually, one could also employ a more common dissimilarity measure like the Jaccard distance or the Dice dissimilarity~\cite{choi2010survey}.
The latter two are even symmetric and normalized to~$[0,1]$.
However, our deselection dissimilarity has two advantages:
First, if $s^{\text{old}}$ is constant, the dissimilarity is \emph{linear} in $s^{\text{new}}$, as it amounts to a simple sum, even if the exact number of newly selected features is unknown yet.
This property is useful in solver-based search (cf.~Section~\ref{sec:csd:approach:alternatives:smt}).
In contrast, Jaccard distance and Dice dissimilarity involve a fraction and are therefore non-linear.
Second, the constraint from Equation~\ref{eq:csd:constraint-dissimilarity} is \emph{antimonotonic}~\cite{ng1998exploratory} in the new feature selection:
If a feature set satisfies the constraint, all its subsets also satisfy the constraint.
Vice versa, if a feature set violates the constraint, all its supersets also violate the constraint.
This property is useful in heuristic search (cf.~Section~\ref{sec:csd:approach:alternatives:heuristics}).
Using the Jaccard distance or Dice dissimilarity in the constraint may violate the property.
In particular, these dissimilarities can increase by selecting features that were not selected in the existing subgroup.

\subsubsection{SMT Encoding}
\label{sec:csd:approach:alternatives:smt}

We only need to slightly reformulate Equation~\ref{eq:csd:hamming-general} to obtain an objective function that is linear regarding the alternative subgroup-membership vector~$b^{(a)}$:
%
\begin{equation}
	\text{sim}_{\text{nHamm}}(b^{(a)}, b^{(0)}) = \frac{\sum_{i=1}^m \left( b_i^{(a)} \leftrightarrow b_i^{(0)} \right) }{m} = \frac{\sum\limits_{\substack{i \in \{1, \dots, m\} \\ b_i^{(0)} = 1}} b_j^{(a)} + \sum\limits_{\substack{i \in \{1, \dots, m\} \\ b_i^{(0)} = 0}} \lnot b_j^{(a)}}{m}
	\label{eq:csd:smt-hamming}
\end{equation}
%
In particular, since $b^{(0)}$~is known and therefore constant, we employ the expression on the right-hand side, i.e., do not need the logical equivalence operator.
Instead, we compute two sums, one for data objects that are members of the original subgroup and one for non-members.

To formulate the dissimilarity constraints, we leverage that the feature-selection vector~$s^{(l)}$ and the corresponding number of selected features~$k^{(l)}$ are known for all existing subgroups as well.
Thus, we adapt Equation~\ref{eq:csd:constraint-dissimilarity} as follows:
%
\begin{equation}
	\forall l \in \{0, \dots, a-1\}:~ \text{dis}_{\text{des}}(s^{(a)}, s^{(l)}) = \sum_{\substack{j \in \{1, \dots, n\} \\ s^{(l)}_j = 1}} \lnot s^{(a)}_j \geq \min \left( \tau_{\text{abs}},~k^{(l)} \right)
	\label{eq:csd:smt-constraint-dissimilarity}
\end{equation}
%
In particular, we only sum over features that were selected in the existing subgroup and check whether they are deselected.
To tie the variables~$s^{(a)}_j$ to the subgroup's bounds, we use Equation~\ref{eq:csd:smt-constraint-feature-selection}, which we already employed for feature cardinality constraints.

\subsubsection{Integration into Heuristic Search Methods}
\label{sec:csd:approach:alternatives:heuristics}

Beam only, no baselines.
Explain antimonotonic, state that SMT is more flexible

\subsubsection{Computational Complexity}
\label{sec:csd:approach:alternatives:complexity}

\paragraph{Hardness}

(cf.~Appendix~\ref{sec:csd:appendix:proofs:alternatives} for the proof)

\begin{proposition}[Complexity of]
	\label{prop:csd:complexity-alternatives-perfect-subgroup}
\end{proposition}

(cf.~Appendix~\ref{sec:csd:appendix:proofs:alternatives} for the proof)

\begin{proposition}[Complexity of]
	\label{prop:csd:complexity-alternatives-imperfect-subgroup}
\end{proposition}

\paragraph{Parameterized complexity}

\begin{proposition}[Parameterized complexity of]
	\label{prop:csd:complexity-alternatives-xp}
\end{proposition}

\section{Related Work}
\label{sec:csd:related-work}

In this section, we review related work.
Next to literature on subgroup discovery (cf.~Section~\ref{sec:csd:related-work:subgroup-discovery}), we also discuss relevant work from the adjacent field of feature selection (cf.~Section~\ref{sec:csd:related-work:feature-selection}) and other related areas (cf.~Section~\ref{sec:csd:related-work:other}).

\subsection{Subgroup Discovery}
\label{sec:csd:related-work:subgroup-discovery}

PRIM~\cite{friedman1999bump}, BI~\cite{mampaey2012efficient}
depth-first~\cite{millot2020optimal}
exhaustive~\cite{atzmueller2006sd, atzmueller2009fast, grosskreutz2009subgroup, lemmerich2016fast} -- but may use pruning, e.g., \cite{grosskreutz2009subgroup} defines quality constraints for refinement
exhaustive with diversity~\cite{bosc2018anytime, lemmerich2010fast}
if SMT formulation is novel, should say so
heuristics with diversity \cite{leeuwen2012diverse, lucas2018ssdp+, proencca2022robust}
kind of diversity (term not directly used) by downweighting (instead of removing) positive data objects covered by previous subgroups \cite{lavrac2004subgroup}
\cite{atzmueller2015subgroup} discusses different from of subgroup set selection
post-processing set of subgroups (four criteria based on data object overlap or prediction performance) \cite{knobbe2006pattern}
relevance/dominance for filtering subgroups \cite{grosskreutz2012enhanced}
analyze Pareto front for diversity (exact and greedy) \cite{leeuwen2013discovering}
diverse subgroup lists \cite{lopez2023discovering, lopez2023novel}
diverse rule sets \cite{zhang2020diverse}
heuristic (beam search) with constraints \cite{lavravc2006relevancy} -- has feature-cardinality constraints, but does not vary them; also, rather specific dataset (gene expression)
min and max num of data objects in beam search \cite{meeng2021real}
contrasting SD \cite{langohr2013contrasting}
involve domain experts/knowledge \cite{dzyuba2013interactive, gamberger2002expert, lemmerich2011local}
background knowledge \cite{atzmueller2005exploiting, atzmueller2006methodological}
`propositionalization to describe subgroups' \cite{zelezny2006propositionalization}

from chair: \cite{arzamasov2021reds} \cite{arzamasov2022pedagogical} \cite{vollmer2019informative}

minimizing subgroups NP-hard~\cite{boley2009non}

measures of subgroup complexity \cite{helal2016subgroup, herrera2011overview, ventura2018subgroup} -> num subgroups and num selected features

\subsection{Feature Selection}
\label{sec:csd:related-work:feature-selection}

\cite{bach2022empirical} \cite{bach2023finding, bach2024alternative}

\subsection{Other Fields}
\label{sec:csd:related-work:other}

\cite{bailey2014alternative} \cite{grossi2017survey}
\cite{guidotti2022counterfactual}
\cite{narodytska2018learning} \cite{schidler2021sat} \cite{yu2021learning}

\section{Experimental Design}
\label{sec:csd:experimental-design}

In this section, we introduce our experimental design.
After a brief overview of its components (cf.~Section~\ref{sec:csd:experimental-design:overview}), we describe evaluation metrics (cf.~Section~\ref{sec:csd:experimental-design:metrics}), subgroup-discovery methods (cf.~Section~\ref{sec:csd:experimental-design:methods}), experimental scenarios (cf.~Section~\ref{sec:csd:experimental-design:scenarios}), and datasets (cf.~Section~\ref{sec:csd:experimental-design:datasets}).
Finally, we shortly outline our implementation (cf.~Section~\ref{sec:csd:experimental-design:implementation}).

\subsection{Overview}
\label{sec:csd:experimental-design:overview}

In our experiments, we evaluate six subgroup-discovery methods on 27 binary-classification datasets.
We measure the subgroup quality in terms of nWRAcc and also record the runtime of the methods.
We analyze four \emph{experimental scenarios}:
First, we compare all subgroup-discovery methods without constraints.
Second, we vary the timeout in solver-based search.
Third, we compare all subgroup-discovery methods with a constraint on the number of selected features~$k$, varying the latter parameter.
Fourth, we search for alternative subgroup descriptions with one solver-based and one heuristic search method.
We vary the number of alternatives~$a$ and the dissimilarity threshold~$\tau_{\text{abs}}$.

\subsection{Evaluation Metrics}
\label{sec:csd:experimental-design:metrics}

\paragraph{Subgroup quality}

We use \emph{nWRAcc} (cf.~Equation~\ref{eq:csd:wracc-normalized}) to quantify subgroup quality.
To analyze how well the subgroup-discovery methods generalize, we conduct a stratified five-fold cross-validation.
In particular, each run of a subgroup-discovery method only uses 80\% of the data objects of a dataset as training data, while the remaining data objects serve as test data.
Based on the bounds of each found subgroup, we determine subgroup membership for data objects from training \emph{and} test data.
Based on this membership information and the true class labels~$y$, we compute \emph{training-set nWRAcc} and \emph{test-set nWRAcc} on the corresponding part of the data separately.

\paragraph{Subgroup similarity}

For evaluating alternative subgroup descriptions, we not only consider their quality but also their similarity to the original subgroup.
To this end, we use \emph{normalized Hamming similarity} (cf.~Equation~\ref{eq:csd:hamming-general}) and \emph{Jaccard similarity} (cf.~Equation~\ref{eq:csd:jaccard}) to compare subgroup membership of data objects between the original subgroup and an alternative subgroup description.

\paragraph{Runtime}

As \emph{runtime}, we consider the training time of the subgroup-discovery methods.
In particular, we measure how long the search for each subgroup takes.
In solver-based search for subgroups, we also register whether the solver timed or not.
In the latter case, the found solution is optimal, at least on the training set; in the former case, it may be suboptimal.

\subsection{Subgroup-Discovery Methods}
\label{sec:csd:experimental-design:methods}

We employ six subgroup-discovery methods:
Our solver-based one (cf.~Section~\ref{sec:csd:approach:smt}), three heuristic search methods from related work (cf.~Section~\ref{sec:csd:fundamentals:heuristics}), and our two baselines (cf.~Section~\ref{sec:csd:fundamentals:baselines}).

\paragraph{Solver-based search}

For solver-based search, we use an SMT optimizer to solve our \emph{SMT} encoding of subgroup discovery (cf.~Equation~\ref{eq:csd:smt-problem-unconstrained-complete}).
Unlike the other five subgroup-discovery methods in our experiments, this method is theoretically guaranteed to find the exact global optimum, though only if it is granted sufficient time.
In practice, however, we set solver timeouts to keep the runtime under control (cf.~Section~\ref{sec:csd:experimental-design:scenarios}).

\paragraph{Heuristic search}

We evaluate three heuristic search methods from related work:
\emph{PRIM} (cf.~Algorithm~\ref{al:csd:prim}), \emph{Beam Search} (cf.~Algorithm~\ref{al:csd:beam-search}, subsequently called \emph{Beam}), and \emph{Best Interval} (cf.~Algorithm~\ref{al:csd:best-interval}, subsequently called \emph{BI}).
For all three methods, we use WRAcc (cf.~Equation~\ref{eq:csd:wracc}) as the subgroup-quality function~$Q(\mathit{lb}, \mathit{ub}, X, y)$.
We set the peeling fraction of PRIM to~$\alpha = 0.05$, consistent with other implementations of PRIM~\cite{arzamasov2021reds, kwakkel2017the}, and the support threshold to~$\beta_0 = 0$, so the shrinking of the subgroup is solely limited by WRAcc improvement and not the subgroup size.
For \emph{Beam} and \emph{BI}, we choose a beam width of $w=10$, falling between default values used in other implementations~\cite{arzamasov2021reds, lemmerich2019pysubgroup}.

\paragraph{Baselines}

We also include baselines, which are even simpler than the heuristic search methods.
In particular, we employ our own methods \emph{MORB} (cf.~Algorithm~\ref{al:csd:morb}) and \emph{Random Search} (cf.~Algorithm~\ref{al:csd:random-search}, subsequently called \emph{Random}).
\emph{MORB} is parameter-free.
For \emph{Random}, we set the number of iterations to~$n\_iters = 1000$ and use WRAcc (cf.~Equation~\ref{eq:csd:wracc}) as the subgroup-quality function~$Q(\mathit{lb}, \mathit{ub}, X, y)$.

\subsection{Experimental Scenarios}
\label{sec:csd:experimental-design:scenarios}

We evaluate the subgroup-discovery methods in four experimental scenarios.
Two of the scenarios do not involve all subgroup-discovery methods.

\paragraph{Unconstrained subgroup discovery}

Our first experimental scenario (cf. Section~\ref{sec:csd:evaluation:unconstrained} for results) compares all six subgroup-discovery methods without any constraints.
This comparison allows us to assess the effectiveness of the solver-based search method \emph{SMT} for `conventional' subgroup discovery and serves as a reference point for subsequent experiment with constraints.

\paragraph{Solver timeouts}

Our second experimental scenario (cf.~Section~\ref{sec:csd:evaluation:timeouts} for results) takes a deeper dive into \emph{SMT} as subgroup-discovery method.
In particular, we analyze whether setting solver timeouts enables finding solutions with reasonable quality in a shorter time frame.
If the solver does not finish within a given timeout, we record the currently best solution at this time, which may be suboptimal.
We evaluate twelve exponentially scaled timeout values, i.e., \{1~s, 2~s, 4~s, $\dots$, 2048~s\}.
In the three other experimental scenarios, we employ the maximum timeout of 2048~s for \emph{SMT}.
Since the heuristic search methods and baselines are significantly faster, we do not conduct a timeout analysis for them.

\paragraph{Feature-cardinality constraints}

Our third experimental scenario (cf.~Section~\ref{sec:csd:evaluation:cardinality} for results) analyzes feature-cardinality constraints (cf.~Section~\ref{sec:csd:approach:cardinality}) for all six subgroup-discovery methods.
In particular, we evaluate $k \in \{1, 2, 3, 4, 5\}$ selected features.
These values of~$k$ are upper bounds (cf.~Equation~\ref{eq:csd:smt-constraint-feature-cardinalty}), i.e., the subgroup-discovery methods may select less features if selecting more does not improve subgroup quality.

\paragraph{Alternative subgroup descriptions}

Our fourth experimental scenario (cf. Section~\ref{sec:csd:evaluation:alternatives} for results) studies alternative subgroup descriptions (cf.~Section~\ref{sec:csd:approach:alternatives}) for \emph{SMT} and \emph{Beam}, i.e., one solver-based and one heuristic search method.
We limit the number of selected features to~$k=3$, which yields reasonably high subgroup quality (cf.~Section~\ref{sec:csd:evaluation:cardinality}).
We search for $a=5$ alternative subgroup descriptions with a dissimilarity threshold $\tau_{\text{abs}} \in \{1, 2, 3\}$.
Since each dataset has $n \geq 20$ features (cf.~Section~\ref{sec:csd:experimental-design:datasets}), our choices of~$a$, $k$, and~$\tau$ ensure that there always is a valid alternative.

\begin{table}[p]
	\centering
	\begin{tabular}{lrrll}
		\toprule
		\multirow{2}{*}{Dataset} & \multirow{2}{*}{$m$} & \multirow{2}{*}{$n$} & \multicolumn{2}{c}{Timeouts} \\
		\cmidrule(lr){4-5}
		& & & Max~$k$ & Any~$k$ \\
		\midrule
		backache & 180 & 32 & No & No \\
		chess & 3196 & 36 & No & No \\
		churn & 5000 & 20 & Yes & Yes \\
		clean1 & 476 & 168 & No & No \\
		clean2 & 6598 & 168 & No & No \\
		coil2000 & 9822 & 85 & Yes & Yes \\
		credit\_g & 1000 & 20 & Yes & Yes \\
		dis & 3772 & 29 & No & No \\
		GE\_2\_Way\_20atts\_0.1H\_EDM\_1\_1 & 1600 & 20 & Yes & Yes \\
		GE\_2\_Way\_20atts\_0.4H\_EDM\_1\_1 & 1600 & 20 & No & No \\
		GE\_3\_Way\_20atts\_0.2H\_EDM\_1\_1 & 1600 & 20 & Yes & Yes \\
		GH\_20atts\_1600\_Het\_0.4\_0.2\_50\_EDM\_2\_001 & 1600 & 20 & Yes & Yes \\
		GH\_20atts\_1600\_Het\_0.4\_0.2\_75\_EDM\_2\_001 & 1600 & 20 & Yes & Yes \\
		Hill\_Valley\_with\_noise & 1212 & 100 & Yes & Yes \\
		horse\_colic & 368 & 22 & No & No \\
		hypothyroid & 3163 & 25 & No & No \\
		ionosphere & 351 & 34 & Yes & Yes \\
		molecular\_biology\_promoters & 106 & 57 & No & No \\
		mushroom & 8124 & 22 & No & No \\
		ring & 7400 & 20 & Yes & Yes \\
		sonar & 208 & 60 & No & Yes \\
		spambase & 4601 & 57 & No & Yes \\
		spect & 267 & 22 & No & No \\
		spectf & 349 & 44 & No & Yes \\
		tokyo1 & 959 & 44 & No & Yes \\
		twonorm & 7400 & 20 & Yes & Yes \\
		wdbc & 569 & 30 & No & No \\
		\bottomrule
	\end{tabular}
	\caption{
		Datasets from PMLB used in our experiments.
		$m$~denotes the number of data objects and $n$~the number of features.
		In dataset names, we replaced \emph{GAMETES\_Epistasis} with  \emph{GE\_} and \emph{GAMETES\_Heterogeneity} with \emph{GH\_} to reduce the table's width.
		The column \emph{Timeouts} indicates whether at least one timeout occurred with \emph{SMT} as the subgroup-discovery method and the highest timeout (2048~s), optimizing the original subgroup without cardinality constraints (\emph{Max~$k$}) or in any cardinality setting (\emph{Any~$k$}).
	}
	\label{tab:csd:datasets}
\end{table}

\subsection{Datasets}
\label{sec:csd:experimental-design:datasets}

We use binary-classification datasets from the Penn Machine Learning Benchmarks (PMLB)~\cite{olson2017pmlb, romano2021pmlb}.
If both classes occur with different frequencies, we encode the minority class as the class of interest, i.e., assign~1 as it class label.
To avoid prediction scenarios that may be too easy or have not enough features for alternative subgroup descriptions, we only select datasets with at least 100 data objects and 20 features.
Next, we exclude one dataset with 1000 features, which has a significantly higher dimensionality than all remaining datasets.
Finally, we manually exclude datasets that seem duplicated or modified versions of other datasets in our experiments.

Based on these criteria, we obtain 27 datasets with 106 to 9822 data objects and 20 to 168 features (cf.~Table~\ref{tab:csd:datasets}).
The datasets do not contain any missing values.
Further, there are no categorical feature values since PMLB encodes them ordinally by default.

\subsection{Implementation and Execution}
\label{sec:csd:experimental-design:implementation}

We implemented all subgroup-discovery methods, experiments, and evaluations in Python~3.8.
A requirements file in our repository specifies the versions of all Python packages.
The solver \emph{Z3}~\cite{bjorner2015nuz, deMoura2008z3} carries out the SMT optimization.
The experimental pipeline parallelizes over datasets, cross-validation folds, and subgroup-discovery methods, while each of these experimental tasks runs single-threaded.
We ran the pipeline on a server with 128~GB RAM and an \emph{AMD EPYC 7551} CPU, having 32~physical cores and a base clock of 2.0~GHz.
With this hardware, the parallelized pipeline run took approximately 39~hours.

\section{Evaluation}
\label{sec:csd:evaluation}

In this section, we evaluate our experiments.
In particular, we cove our four experimental scenarios, i.e., unconstrained subgroup discovery (cf.~Section~\ref{sec:csd:evaluation:unconstrained}), solver timeouts (cf.~Section~\ref{sec:csd:evaluation:timeouts}), feature-cardinality constraints (cf.~Section~\ref{sec:csd:evaluation:cardinality}), and alternative subgroup descriptions. (cf.~Section~\ref{sec:csd:evaluation:alternatives}).
Finally, we summarize key experimental results (cf.~Section~\ref{sec:csd:evaluation:summary}).

\subsection{Unconstrained Subgroup Discovery}
\label{sec:csd:evaluation:unconstrained}

\paragraph{Training-set quality}

\paragraph{Test-set quality}

\paragraph{Overfitting}

\paragraph{Runtime}

\subsection{Solver Timeouts}
\label{sec:csd:evaluation:timeouts}

\subsection{Feature-Cardinality Constraints}
\label{sec:csd:evaluation:cardinality}

\paragraph{Quality}

\paragraph{Runtime}

\subsection{Alternative Subgroup Descriptions}
\label{sec:csd:evaluation:alternatives}

\paragraph{Quality}

\paragraph{Runtime}

\subsection{Summary}
\label{sec:csd:evaluation:summary}

\paragraph{Unconstrained subgroup discovery (cf.~Section~\ref{sec:csd:evaluation:unconstrained})}

\paragraph{Solver timeouts (cf.~Section~\ref{sec:csd:evaluation:timeouts})}

\paragraph{Feature-cardinality constraints (cf.~Section~\ref{sec:csd:evaluation:cardinality})}

\paragraph{Alternative subgroup descriptions (cf.~Section~\ref{sec:csd:evaluation:alternatives})}

\section{Conclusions and Future Work}
\label{sec:csd:conclusion}

In this section, we recap our article (cf.~Section~\ref{sec:csd:conclusion:conclusion}) and propose directions for future work (cf.~Section~\ref{sec:csd:conclusion:future-work}).

\subsection{Conclusions}
\label{sec:csd:conclusion:conclusion}

Subgroup-discovery methods constitute an important category of interpretable machine-learning models.
However, they often lack the support of user-defined or user-parameterized constraints.
In this article, we analyzed constrained subgroup discovery.
First, we formalized subgroup discovery as an SMT optimization problem.
This formulation gives way to a variety of user constraints and enables solver-based search for subgroups.
In particular, we studied two constraint types, i.e., limiting the number of features used in subgroups and searching for alternative subgroup descriptions.
For the latter constraint type, we let users control the number of alternatives and a dissimilarity threshold.
We showed how to integrate these constraint types into our SMT formulation as well as existing heuristic search methods for subgroup discovery.
Further, we proved $\mathcal{NP}$-hardness of the optimization problem with constraints.
Finally, we evaluated heuristic and solver-based search with 27 binary-classification datasets.
In particular, we analyzed four experimental scenarios:
unconstrained subgroup discovery, our two constraint types, and timeouts for solver-based search.

\subsection{Future Work}
\label{sec:csd:conclusion:future-work}

\paragraph{Datasets}

Our evaluation used over two dozen generic benchmark datasets (cf.~Section~\ref{sec:csd:experimental-design:datasets}).
While such an evaluation shows general trends, the impact of constraints naturally depends on the dataset.
Thus, our results may not transfer to each particular scenario.
This caveat calls for domain-specific case studies.
In such studies, one could also interpret alternative subgroup descriptions qualitatively, i.e., from the domain perspective.

\paragraph{Constraint types}

We formalized, analyzed, and evaluated two constraint types, i.e., feature-cardinality constraints (cf.~Sections~\ref{sec:csd:approach:cardinality} and~\ref{sec:csd:evaluation:cardinality}) and alternative subgroup descriptions (cf.~Sections~\ref{sec:csd:approach:alternatives} and~\ref{sec:csd:evaluation:alternatives}).
As mentioned in Section~\ref{sec:csd:approach:constraint-types}, there are further constraint types one could investigate, e.g., domain-specific constraints, secondary objectives, or alternatives in the sense of covering different data objects rather than covering the same data objects differently.

For our constraint type for alternative subgroup descriptions, one could analyze other dissimilarities, e.g., symmetric ones rather than the asymmetric one we used (cf.~Equation~\ref{eq:csd:constraint-dissimilarity}).
While the SMT encoding of subgroup discovery is relatively flexible regarding dissimilarities, integrating them into heuristic search methods may be challenging, e.g., if they are not antimonotonic.

\paragraph{Formalization}

In solver-based search for subgroups, we used an SMT encoding (cf.~Section~\ref{sec:csd:approach:smt}) and one particular solver.
Different white-box encodings or solvers may speed up the search and lead to less timeouts, potentially improving the subgroup quality.
We already proposed MILP and MaxSAT encodings of subgroup discovery (cf.~Appendix~\ref{sec:csd:appendix:further-encodings}), though without evaluation.

Two assumptions for subgroup discovery in our paper were numerical features and a binary target (cf.~Section~\ref{sec:csd:fundamentals:notation}).
One could adapt the SMT encoding to multi-valued categorical features (cf.~Appendix~\ref{sec:csd:appendix:further-encodings}) and continuous targets.

\paragraph{Computational complexity}

We established $\mathcal{NP}$-hardness for optimizing perfect and imperfect subgroups with feature-cardinality constraints (cf.~Propositions~\ref{prop:csd:complexity-cardinality-perfect-subgroup} and~\ref{prop:csd:complexity-cardinality-imperfect-subgroup}).
While the problem admits a polynomial-time algorithm for perfect subgroups without such constraints (cf.~Proposition~\ref{prop:csd:complexity-unconstrained-perfect-subgroup}), we did not analyze the general unconstrained problem, i.e., including imperfect subgroups.

Further, we showed $\mathcal{NP}$-hardness for finding alternative subgroup descriptions for perfect and imperfect subgroups (cf.~Propositions~\ref{prop:csd:complexity-alternatives-perfect-subgroup} and~\ref{prop:csd:complexity-alternatives-imperfect-subgroup}).
In both cases, our proofs tackle the decision problem whether perfect alternatives exist, i.e., alternative descriptions that entail exactly the same subgroup membership of data objects as the original subgroup.
One could try to extend these proofs to imperfect alternatives, or generally speaking, optimizing alternatives.

Regarding parameterized complexity for the unconstrained scenario, feature-cardinality constraints, and alternative subgroup descriptions, we proved membership in the relatively broad complexity class $\mathcal{XP}$ (cf.~Propositions~~\ref{prop:csd:complexity-unconstrained-xp}, \ref{prop:csd:complexity-cardinality-xp}, and~\ref{prop:csd:complexity-alternatives-xp}).
One may attempt to tighten these results.

Finally, while we described how one can integrate feature-cardinality constraints and alternative subgroup descriptions into heuristic search methods (cf.~Sections~\ref{sec:csd:approach:cardinality:heuristics} and~\ref{sec:csd:approach:alternatives:heuristics}), we did not provide quality guarantees relative to the exact optimum.
In that regard, one could seek for an approximation complexity result, e.g., membership in the complexity class~$\mathcal{APX}$.

%~\\
%\noindent \textsc{Acknowledgments}\quad
%This work was supported by the Ministry of Science, Research and the Arts Baden-Wrttemberg, project \emph{Algorithm Engineering for the Scalability Challenge (AESC)}.

\appendix

\section{Appendix}
\label{sec:csd:appendix}

In this section, we provide supplementary materials.
Appendix~\ref{sec:csd:appendix:further-encodings} describes further encodings of the subgroup-discovery problem, complementing the SMT encoding from Section~\ref{sec:csd:approach:smt}.
Appendix~\ref{sec:csd:appendix:proofs} contains proofs for propositions from Section~\ref{sec:csd:approach}.

\subsection{Further Problem Encodings of Subgroup Discovery}
\label{sec:csd:appendix:further-encodings}

In this section, we provide additional encodings of subgroup-discovery as a white-box optimization problem.
First, we describe how to encode categorical features within the SMT formulation (cf.~Section~\ref{sec:csd:appendix:further-encodings:smt-categorical}).
Next, we discuss encodings as a mixed-integer linear program (cf.~Section~\ref{sec:csd:appendix:further-encodings:milp}) and a maximum-satisfiability problem (cf.~Section~\ref{sec:csd:appendix:further-encodings:max-sat}).

\subsubsection{Handling Categorical Features in SMT Encoding}
\label{sec:csd:appendix:further-encodings:smt-categorical}

There are different options to consider categorical features in the SMT optimization problem.
We present three of them in the following.

\paragraph{Two variables per categorical feature}

As a straightforward option, one may map all categories, i.e., unique values, of each categorical feature to distinct integers before instantiating the optimization problem.
This so-called ordinal encoding allows us to directly apply our existing SMT formulation (cf.~Equation~\ref{eq:csd:smt-problem-unconstrained-complete}) to the dataset, at least technically.
Consequently, there are two integer-valued bound variables for each encoded categorical feature.
However, the ordering of categories should be semantically meaningful since it influences which categories may jointly be included in the subgroup.
In particular, only sets of categories that form contiguous integer ranges in the ordinal encoding may define subgroup membership.
I.e., the subgroup may comprise the encoded categories~$\{3,4,5\}$, but not only~$\{3,5 \}$ since it needs to include all values between a lower and an upper bound.
If there is no meaningful ordering of categories, one should choose a different encoding.

\paragraph{Two variables per categorical feature value}

One can achieve more flexibility by introducing separate bound variables for each category of a feature rather than only for each feature.
This approach corresponds to a one-hot encoding of the dataset, which creates one new binary feature for each category.
Thus, the bound variables effectively are binary as well.
By default in our SMT encoding, there is a logical AND ($\land$) over the binary features, i.e., categories.
The interpretation of bound values for one binary Feature~$j$ is as follows:

(Case 1) $\mathit{lb}_j = \mathit{ub}_j = 1$ means that data objects that assume the corresponding category for Feature~$j$ are members of the subgroup.
This case should apply to at most one category of each feature.
Otherwise, the AND operator would require each data object to assume multiple categories for one feature, which is unsatisfiable.
Thus, this encoding cannot directly express that a set of categories is included the subgroup.

(Case 2) $\mathit{lb}_j = \mathit{ub}_j = 0$ means that data objects that do \emph{not} assume the corresponding category for Feature~$j$ are members of the subgroup.
I.e., data objects assuming the corresponding category are not members of the subgroup.
Other than Case~1, this case can apply to multiple categories of each feature, i.e. the subgroup may directly exclude multiple categories.
Further, if one category is actively included in the subgroup, i.e., falls into Case~1, then Case-2 bounds on other categories are redundant since they are implied by the former.

(Case 3) $\mathit{lb}_j = 0,~\mathit{ub}_j = 1$ explicitly deselects a binary feature, i.e., both binary values do not restrict subgroup membership.

(Case 4) $\mathit{lb}_j = 1,~\mathit{ub}_j = 0$ cannot occur since it violates the bound constraints (cf.~Equation~\ref{eq:csd:smt-constraint-bounds-monotonic}).

Finally, note that binary features allow us to slightly simplify the subgroup-membership expression (cf.~Equation~\ref{eq:csd:smt-constraint-subgroup-membership}).
In general, we need to check lower bound and upper bound for a feature.
However, if a binary feature assumes the value~0 for a data object, checking the upper bound is unnecessary since it is always satisfied.
Similarly, if a binary feature assumes the value~1 for a data object, checking the lower bound is unnecessary since it is always satisfied.
Both these simplifications assume that the bounds are explicitly defined as binary or at least in $[0, 1]$, which can be enforced with straightforward constraints.
Otherwise, the bounds may theoretically be placed outside the feature's range and exclude all data objects, producing an empty subgroup.

\paragraph{One variable per categorical feature value}

In some scenario, it does not make sense to include the absence of a category in the subgroup, i.e., to permit $\mathit{lb}_j = \mathit{ub}_j = 0$.
In particular, some existing subgroup-discovery methods for categorical data assume that only the presence of categories is interesting~\cite{atzmueller2015subgroup}.
In this case, introducing one instead of two bound variable(s) for each category suffices.
Assume the categorical Feature~$j$ has $|c_j| \in \mathbb{N}$ different categories~$\{c^1_j, \dots, c^{|c_j|}_j\}$.
Let $\mathit{cb}_j \in \{0, 1\}^{|c_j|}$ denote the corresponding bound variables, which denote whether a category is included in the subgroup.
The ordering of categories in this vector is arbitrary but fixed.

As a difference to previously described encodings, the subgroup-membership expression (cf.~Equation~\ref{eq:csd:smt-constraint-subgroup-membership}) should still use a logical AND ($\land$) over features but not over categories belonging to the same feature.
Otherwise, the expression would be unsatisfiable, since each data object only assumes one category for each feature.
Instead, we replace the numeric bound check~$\left( X_{ij} \geq \mathit{lb}_j \right) \land \left( X_{ij} \leq \mathit{ub}_j \right)$ for Feature~$j$ with the following OR ($\lor$) expression:
%
\begin{equation}
	\bigvee_{l \in \{1, \dots, |c_j|\}} \left( \mathit{cb}^l_j \land \left(  X_{ij} = c^l_j \right) \right)
	\label{eq:csd:category-constraint:or}
\end{equation}
%
Since the equality holds for exactly one category, all conjunctions except one are false and the expression simplifies to one variable~$\mathit{cb}^{l'}_j$, where $l'$ is the index of the category~$X_{ij}$.
I.e., for each categorical feature, a data object can only be member of the subgroup if the variable belonging to its category is~1.

In general, multiple $\mathit{cb}^l_j$ for Feature~$j$ may be~1, representing multiple categories included in the subgroup, which is an advantage over the previous encoding.
If all categories are in the subgroup, the feature becomes deselected.
Thus, for a categorical Feature~$j$, Equation~\ref{eq:csd:smt-constraint-feature-selection} for feature selection becomes:
%
\begin{equation}
	s_j \leftrightarrow \lnot \bigwedge_{l \in \{1, \dots, |c_j|\}} \mathit{cb}^l_j
	\label{eq:csd:category-constraint:feature-selection}
\end{equation}
%
One can also constrain the number of categories in the subgroup, e.g., to either include one category of Feature~$j$ in the subgroup or deselect the feature altogether by including all categories:
%
\begin{equation}
	 \left( \left( \sum_{l=1}^{|c_j|} \mathit{cb}^l_j \right) = 1 \right) \lor \left( \left( \sum_{l=1}^{|c_j|} \mathit{cb}^l_j \right) = |c_j| \right)
	\label{eq:csd:category-constraint:cardinality}
\end{equation}

\subsubsection{Mixed-Integer Linear Programming (MILP)}
\label{sec:csd:appendix:further-encodings:milp}

The MILP formulation extends the SMT formulation.
Additional variables and constraints are necessary to linearize certain logical expressions and operators.

\paragraph{Unconstrained subgroup discovery}

From the corresponding SMT formulation (cf.~Equation~\ref{eq:csd:smt-problem-unconstrained-complete}), we can keep all decision variables:
the binary variables~$b_i$ for subgroup membership and the real-valued bound variables~$\mathit{lb}_j$ and~$\mathit{ub}_j$.
The bound constraints (cf.~Equation~\ref{eq:csd:smt-constraint-bounds-monotonic}) remain unchanged as well.
Further, we retain the optimization objective, which already is linear in~$b_i$ (cf.~Equations~\ref{eq:csd:smt-wracc} and~\ref{eq:csd:smt-constraint-m-as-sum}).
However, we need to linearize the logical AND operators ($\land$) in the definition of subgroup membership~$b_i$ (cf.~Equation~\ref{eq:csd:smt-constraint-subgroup-membership}) by introducing auxiliary variables and further constraints.
In particular, we supplement the variables~$b \in \{0, 1\}^n$ by $b^{\text{lb}} \in \{0, 1\}^{n \times m}$ and $b^{\text{ub}} \in \{0, 1\}^{n \times m}$.
These new binary variables indicate whether a particular data object satisfies the lower respectively upper bound for a particular feature.
Using linearization techniques for constraint satisfaction and AND operators from~\cite{mosek2022modeling}, we obtain the following set of constraints to replace Equation~\ref{eq:csd:smt-constraint-subgroup-membership}:
%
\begin{equation}
	\begin{aligned}
		\forall i~\forall j: & & X_{ij} + m_j \cdot b^{\text{lb}}_{ij} &\leq \mathit{lb}_j - \varepsilon_j \\
	 	\forall i~\forall j: & & \mathit{lb}_j &\leq X_{ij} + M_j \cdot \left(1 - b^{\text{lb}}_{ij} \right) \\
	 	\forall i~\forall j: & & \mathit{ub}_j + m_j \cdot b^{\text{ub}}_{ij} &\leq X_{ij} - \varepsilon_j \\
	 	\forall i~\forall j: & & X_{ij} &\leq \mathit{ub}_j + M_j \cdot \left(1 - b^{\text{ub}}_{ij} \right) \\
	 	\forall i~\forall j: & & b_i &\leq b^{\text{lb}}_{ij} \\
	 	\forall i~\forall j: & & b_i &\leq b^{\text{ub}}_{ij} \\
	 	\forall i: & & \sum_{j=1}^{n} \left( b^{\text{lb}}_{ij} + b^{\text{ub}}_{ij} \right) &\leq b_i + 2n - 1 \\
		\text{with indices:} & & i &\in \{1, \dots, m\} \\
		& & j &\in \{1, \dots, n\}
	\end{aligned}
	\label{eq:csd:milp-constraint-subgroup-membership}
\end{equation}
%
The first two inequalities ensure that $b^{\text{lb}}_{ij} = 1$ if and only if $\mathit{lb}_j \leq X_{ij}$.
The next two inequalities perform a corresponding check for~$b^{\text{ub}}_{ij}$.
The values~$\varepsilon_j \in \mathbb{R}_{> 0}$ are small constants that turn strict inequalities into non-strict inequalities since a MILP solver may only be able to handle the latter.
One possible choice, which we used in a demo implementation, is sorting all unique feature values and taking the minimum difference between two consecutive values in that order.

The values~$M_j \in \mathbb{R}_{> 0}$ and $m_j \in \mathbb{R}_{< 0}$ are large positive and negative constants, respectively.
They allow us to express logical implications between real-valued and binary-valued expressions, compensating for the fact that the latter may have a considerably smaller range.
One potential choice for~$M_j$ is taking a value larger than the difference between the feature's minimum and maximum, which can be pre-computed before optimization:
%
\begin{equation}
	\begin{aligned}
		\forall j \in \{1, \dots, n\} & & M_j &:= 2 \cdot \left( \max_{i \in \{1, \dots, m\}} X_{ij} - \min_{i \in \{1, \dots, m\}} X_{ij} \right) \\
		\forall j \in \{1, \dots, n\} & & m_j &:= 2 \cdot \left( \min_{i \in \{1, \dots, m\}} X_{ij} - \max_{i \in \{1, \dots, m\}} X_{ij} \right) \\
	\end{aligned}
	\label{eq:csd:milp-big-m}
\end{equation}
%
In particular, the difference between the subgroup's bounds and arbitrary feature values must be smaller than $M_j$ and larger than $m_j$, unless the bounds are placed outside the feature's value range.
Since the latter does not improve the subgroup's quality in any case, we prevent it with additional constraints on the bound variables~$\mathit{lb}_j$ and~$\mathit{ub}_j$:
%
\begin{equation}
	\begin{aligned}
		\forall j \in \{1, \dots, n\} & & \min_{i \in \{1, \dots, m\}} X_{ij} &\leq \mathit{lb}_j &\leq \max_{i \in \{1, \dots, m\}} X_{ij} \\
		\forall j \in \{1, \dots, n\} & & \min_{i \in \{1, \dots, m\}} X_{ij} &\leq \mathit{ub}_j &\leq \max_{i \in \{1, \dots, m\}} X_{ij} \\
	\end{aligned}
	\label{eq:csd:milp-constraint-bounds-in-range}
\end{equation}
%
Finally, the last three inequalities in Equation~\ref{eq:csd:milp-constraint-subgroup-membership} tie $b^{\text{lb}}_{ij}$ and $b^{\text{ub}}_{ij}$ to $b_i$ and linearize the logical AND operators ($\land$) from Equation~\ref{eq:csd:smt-constraint-subgroup-membership}.
In particular, these constraints ensure that a object is a member of the subgroup, i.e., $b_i = 1$, if and only if all feature values of the data object observe the lower and upper bounds, i.e., all corresponding $b^{\text{lb}}_{ij} = 1$ and $b^{\text{ub}}_{ij} = 1$.

\paragraph{Feature-cardinality constraints}

The feature-cardinality constraint from the SMT formulation (cf.~Equation~\ref{eq:csd:smt-constraint-feature-cardinalty}) already is a linear expression in the feature-selection variables~$s_j$, so we can keep it as-is.
However, the constraints defining~$s_j$ (cf.~Equation~\ref{eq:csd:smt-constraint-feature-selection}) contain a logical OR operator ($\lor$) and comparison ($<$) expressions.
We linearize these constraints as follows:
%
\begin{equation}
	\begin{aligned}
		\forall i~\forall j: & & 1 - b^{\text{lb}}_{ij} &\leq s^{\text{lb}}_j \\
		\forall i~\forall j: & & 1 - b^{\text{ub}}_{ij} &\leq s^{\text{ub}}_j \\
		\forall j: & & s^{\text{lb}}_j &\leq s_j \\
		\forall j: & & s^{\text{ub}}_j &\leq s_j \\
		\forall j: & & s_j &\leq 2m - \sum_{i=1}^{m} \left( b^{\text{lb}}_{ij} + b^{\text{ub}}_{ij} \right) \\
		\text{with indices:} & & i &\in \{1, \dots, m\} \\
		& & j &\in \{1, \dots, n\}
	\end{aligned}
	\label{eq:csd:milp-constraint-feature-selection}
\end{equation}
%
The first four inequalities ensure that a feature is selected, i.e., $s_j = 1$, if any data object's feature value lies outside the subgroup's bounds, i.e., any $b^{\text{lb}}_{ij} = 0$ or $b^{\text{ub}}_{ij} = 0$.
The last inequality covers the other direction of the logical equivalence, i.e., if a feature is selected, then at least one data object's feature value lies outside the subgroup's bounds.

\paragraph{Alternative subgroup descriptions}

The objective function for alternative subgroup descriptions in the SMT formulation (cf.~Equation~\ref{eq:csd:smt-hamming}) already is linear.
We only need to replace the logical negation operators ($\lnot$):
%
\begin{equation}
	\text{sim}_{\text{nHamm}}(b^{(a)}, b^{(0)}) = \frac{\sum\limits_{\substack{i \in \{1, \dots, m\} \\ b_i^{(0)} = 1}} b_j^{(a)} + \sum\limits_{\substack{i \in \{1, \dots, m\} \\ b_i^{(0)} = 0}} \left( 1 - b_j^{(a)} \right)}{m}
	\label{eq:csd:mip-hamming}
\end{equation}
%
The same replacement also applies to the dissimilarity constraints (cf.~Equation~\ref{eq:csd:smt-constraint-dissimilarity}), which now look as follows:
%
\begin{equation}
	\forall l \in \{0, \dots, a-1\}:~ \text{dis}_{\text{des}}(s^{(a)}, s^{(l)}) = \sum_{\substack{j \in \{1, \dots, n\} \\ s^{(l)}_j = 1}} \left(1 - s^{(a)}_j \right) \geq \min \left( \tau_{\text{abs}},~k^{(l)} \right)
	\label{eq:csd:mip-constraint-dissimilarity}
\end{equation}
%
Otherwise, this expression is linear as well, so no further auxiliary variables or constraints are necessary.

\paragraph{Implementation}

Our published code contains a MILP implementation for unconstrained and feature-cardinality-constrained subgroup discovery.
We use the package \emph{OR-Tools}~\cite{perron2022or-tools} with \emph{SCIP}~\cite{bestuzheva2021scip} as the optimizer.
However, this implementation was (on average) slower than the SMT implementation in preliminary experiments, so we stuck to the latter for our main experiments (cf.~Section~\ref{sec:csd:experimental-design:methods}).

\subsubsection{Maximum Satisfiability (MaxSAT)}
\label{sec:csd:appendix:further-encodings:max-sat}

Our SMT formulation of subgroup discovery with and without constraints uses a combination of propositional logic and linear arithmetic.
However, if all feature values are binary, i.e., $X \in \{0, 1\}^{m \times n}$, we can also define a partial weighted MaxSAT problem \cite{bacchus2021maximum, li2021maxsat}.
This formulation involves hard constraints in propositional logic and an objective function containing weighted clauses, i.e., OR terms.
In our case, it even is a \textsc{Max One}~\cite{khanna1997complete} problem since the `clauses' in the objective are plain binary variables.

\paragraph{Unconstrained subgroup discovery}

For binary feature values, the bound variables $\mathit{lb}_j$ and $\mathit{ub}_j$ become binary rather than real-valued as well.
The subgroup membership variables~$b_i$ were binary already (cf.~Equation~\ref{eq:csd:smt-problem-unconstrained-complete}).
In the hard constraints, all less-or-equal inequalities ($\leq$) become logical implications ($\rightarrow$).
Thus, the bound constraints (cf.~Equation~\ref{eq:csd:smt-constraint-bounds-monotonic}) become:
%
\begin{equation}
	\forall j \in \{1, \dots, n\}:~ \mathit{lb}_j \rightarrow \mathit{ub}_j
	\label{eq:csd:maxsat-constraint-bounds-monotonic}
\end{equation}
%
I.e., if the lower bound is~1, then the upper bound also needs to be~1; otherwise, the upper bound may be~0 or~1.

The subgroup-membership expressions (cf.~Equation~\ref{eq:csd:smt-constraint-subgroup-membership}) turn into:
%
\begin{equation}
	\forall i \in \{1, \dots, m\}:~ b_i\leftrightarrow \bigwedge_{j \in \{1, \dots, n\}} \left( \left( \mathit{lb}_j \rightarrow X_{ij} \right) \land \left( X_{ij} \rightarrow \mathit{ub}_j \right) \right)
	\label{eq:csd:maxsat-constraint-subgroup-membership}
\end{equation}
%
Since all values~$X_{ij}$ are known, we can remove and simplify terms in the definition of~$b_i$.
In particular, if $X_{ij} = 1$, then $\mathit{lb}_j \rightarrow X_{ij}$ is a tautology, which we can remove, and $X_{ij} \rightarrow \mathit{ub}_j$ becomes $\mathit{ub}_j$.
Vice, versa, if $X_{ij} = 0$, then $X_{ij} \rightarrow \mathit{ub}_j$ is a tautology and $\mathit{lb}_j \rightarrow X_{ij}$ becomes $\lnot \mathit{lb}_j$.

Further, having determined the bound values, the final subgroup description can be expressed as a plain conjunction of propositional literals, e.g., $b_i \leftrightarrow \left( X_{i2} \land \lnot X_{i5} \land X_{i6} \right)$.
In particular, there are four cases:
(1)~If $\mathit{lb}_j = 0$ and $\mathit{ub}_j = 1$, then the feature's value does not restrict subgroup membership and therefore does not need to be checked in the final subgroup description.
(2)~If $\mathit{lb}_j = \mathit{ub}_j = 0$, then only $X_{ij} = 0$ is in the subgroup, i.e., a negative literal becomes part of the final subgroup description.
(3)~If $\mathit{lb}_j = \mathit{ub}_j = 1$, then only $X_{ij} = 1$ is in the subgroup, i.e., a positive literal becomes part of the final subgroup description.
(4)~The combination $\mathit{lb}_j = 1$ and $\mathit{ub}_j = 0$ violates the bound constraints and will therefore not appear in a valid solution.

Finally, the objective function already is a weighted sum of the subgroup-membership variables~$b_i$, which form the soft constraints for the problem.
In particular, we can re-formulate Equation~\ref{eq:csd:smt-wracc} as follows:
%
\begin{equation}
	\text{WRACC} = \frac{1}{m} \cdot \sum_{\substack{i \in \{1, \dots, m\} \\ y_i = 1 }} b_i - \frac{m^+}{m^2} \cdot \sum_{i=1}^{m} b_i
	\label{eq:csd:maxsat-wracc}
\end{equation}
%
Thus, for negative data objects, i.e., with $y_i = 0$, the weight is $-m^+ / m^2$.
For positive data objects, i.e., with $y_i = 1$, the weight is $(m - m^+) / m^2$.
Since~$m$ is a constant, we can also multiply with~$m^2$ to obtain integer-valued weights.

\paragraph{Feature-cardinality constraints}

For binary features, the definition of the feature selection variables~$s_j$ (cf.~Equation~\ref{eq:csd:smt-constraint-feature-selection}), which are binary by default, amounts to:
%
\begin{equation}
	\begin{aligned}
		\forall j: & & s^{\text{lb}}_j &\leftrightarrow \left( \mathit{lb}_j \land \lnot \left( \bigwedge_{i \in \{1, \dots, m\}} X_{ij} \right) \right) \\
		\forall j: & &s^{\text{ub}}_j &\leftrightarrow \left( \lnot \mathit{ub}_j \land \left( \bigvee_{i \in \{1, \dots, m\}} X_{ij} \right) \right) \\
		\forall j: & & s_j &\leftrightarrow \left( s^{\text{lb}}_j \lor s^{\text{ub}}_j \right) \\
		\text{with index:} & & j &\in \{1, \dots, n\}
	\end{aligned}
	\label{eq:csd:maxsat-constraint-feature-selection}
\end{equation}
%
I.e., a feature is selected regarding its lower bound if the lower bound is set to~1 and and at least one feature value is~0, i.e., at least one feature value is excluded from the subgroup.
Vice versa, a feature is selected regarding its upper bound if the upper bound is set to~0 and and at least one feature value is~1, i.e., at least one feature value is excluded from the subgroup.
Since all values~$X_{ij}$ are known, we can evaluate the corresponding AND and OR term before optimization.
If a feature is~0 and~1 for at least one data object each, which should usually be the case, Equation~\ref{eq:csd:maxsat-constraint-feature-selection} becomes a much simpler expression:
%
\begin{equation}
		s_j \leftrightarrow \left( \mathit{lb}_j\lor \lnot \mathit{ub}_j \right)
	\label{eq:csd:maxsat-constraint-feature-selections-simplified}
\end{equation}
%

To transform the actual feature-cardinality constraint (cf.~Equation~\ref{eq:csd:smt-constraint-feature-cardinalty}), which sums up the variables~$s_j$ and compares them to a user-defined~$k$, into propositional logic, we can use a cardinality encoding from the literature~\cite{sinz2005towards}.

\paragraph{Alternative subgroup descriptions}

The objective function for alternative subgroup descriptions (cf.~Equation~\ref{eq:csd:smt-hamming}) already is a weighted sum of the subgroup-membership variables~$b_j^{(a)}$.
In particular, for negative data objects, i.e., with $y_i = 0$, the weight of $\lnot b_j^{(a)}$ is~$1 / m$.
For positive data objects, i.e., with $y_i = 1$, the weight of $b_j^{(a)}$ is~$1 / m$.
Since~$m$ is a constant, we can also use~1 as the weight.

We can encode the dissimilarity constraint on the feature selection (cf.~Equation~\ref{eq:csd:smt-constraint-dissimilarity}) with a cardinality encoding from the literature~\cite{sinz2005towards}.

\paragraph{Non-binary features}

While we discussed binary features up to now, we can also encode multi-valued features in a way suitable for a MaxSAT formulation.
In Section~\ref{sec:csd:appendix:further-encodings:smt-categorical}, we already addressed how categorical features may be represented binarily.
For numeric features, we can introduce two binary variables for each numeric value:
Let the numeric Feature~$j$ have $|v_j| \in \mathbb{N}$ distinct values~$\{v^1_j, \dots, v^{|v_j|}_j\}$, with higher superscripts denoting higher values.
Next, let $\mathit{lb}_j \in \{0, 1\}^{|v_j|}$ and $\mathit{ub}_j \in \{0, 1\}^{|v_j|}$ denote the corresponding binary bound variables.
I.e., instead of two bound variables per feature, there are two bound variables for each unique feature value now.
$\mathit{lb}^l_j$ indicates whether the $l$-th unique value of Feature~$j$ is the lower bound.
Vice versa, $\mathit{ub}^l_j$ indicates whether the $l$-th unique value of Feature~$j$ is the upper bound.
In case this encoding generates too many variables, we may discretize the feature first, e.g., by binning its values, and representing each bin by one value, e.g., the bin's mean.

The bound constraints (cf.~Equations~\ref{eq:csd:smt-constraint-bounds-monotonic} and~\ref{eq:csd:maxsat-constraint-bounds-monotonic}) take the following form:
%
\begin{equation}
	\begin{aligned}
		\forall j: & & \sum_{l=1}^{|v_j|} \mathit{lb}^l_j &= 1 \\
		\forall j: & & \sum_{l=1}^{|v_j|} \mathit{ub}^l_j &= 1 \\
		\forall j~ \forall l_1 \in \{1, \dots, |v_j|\}: & & \mathit{ub}^{l_1}_j &\rightarrow \bigvee_{l_2 \in \{1, \dots, l_1\}} \mathit{lb}^{l_2}_j \\
		\text{with index:} & & j &\in \{1, \dots, n\}
	\end{aligned}
	\label{eq:csd:maxsat-numeric-constraint-bounds-monotonic}
\end{equation}
%
The first two constraints ensure that exactly one value of Feature~$j$ is chosen as lower bound and upper bound respectively.
These constraints can be encoded into propositional logic with a cardinality encoding from the literature~\cite{sinz2005towards}.
The third constraint enforces that the value chosen as the lower bound is less than or equal to the value chosen as the upper bound.
Alternatively, one could also formulate that the value chosen as the upper bound is greater than or equal to the value chosen as the lower bound.

We formulate the subgroup-membership expressions (cf.~Equations~\ref{eq:csd:smt-constraint-subgroup-membership} and~\ref{eq:csd:maxsat-constraint-subgroup-membership}) as follows:
%
\begin{equation}
	\forall i \in \{1, \dots, m\}:~ b_i\leftrightarrow \bigwedge_{j \in \{1, \dots, n\}} \left( \left( \bigvee_{\substack{l \in \{1, \dots, \bar{l}\} \\ X_{ij} = v_{\bar{l}} }} \mathit{lb}^l_j \right) \land \left( \bigvee_{\substack{l \in \{\bar{l}, \dots, |v_j|\} \\ X_{ij} = v_{\bar{l}} }} \mathit{ub}^l_j \right) \right)
	\label{eq:csd:maxsat-numeric-constraint-subgroup-membership}
\end{equation}
%
In particular, for a data object to be a member of the subgroup, each feature's lower bound needs to be lower or equal to the actual value~$X_{ij}$, while the upper bound needs to be higher or equal.
For the binary lower-bound variables $\mathit{lb}^l_j$, this means that any of the bound variables representing values lower or equal to $X_{ij}$ needs to be~1; vice versa for the upper bounds.

Finally, for feature-cardinality constraints, we express the definitions of the feature-selection variables~$s_j$ (cf.~Equations~\ref{eq:csd:smt-constraint-feature-selection} and~\ref{eq:csd:maxsat-constraint-feature-selection}) as follows:
%
\begin{equation}
	\begin{aligned}
		\forall j, \text{ with } \max_{i \in \{1, \dots, m\}} X_{ij} = v_{l^{\text{min}}}: & & s^{\text{lb}}_j &\leftrightarrow \left( \bigvee_{l \in \{l^{\text{min}} + 1, \dots, |v_j|\}} \mathit{lb}^l_j \right) \\
		\forall j, \text{ with } \min_{i \in \{1, \dots, m\}} X_{ij} = v_{l^{\text{max}}}: & & s^{\text{ub}}_j &\leftrightarrow \left( \bigvee_{l \in \{ 1, \dots, l^{\text{max}} - 1\}} \mathit{ub}^l_j \right) \\
		\forall j: & & s_j &\leftrightarrow \left( s^{\text{lb}}_j \lor s^{\text{ub}}_j \right) \\
		\text{with index:} & & j &\in \{1, \dots, n\}
	\end{aligned}
	\label{eq:csd:maxsat-numeric-constraint-feature-selection}
\end{equation}
%
In particular, we check whether the bounds correspond to a feature value that is larger than the minimum or smaller than the maximum value of that feature, which indicates whether the bounds exclude at least one data object from the subgroup or not.
The actual feature-cardinality constraint (cf.~Equation~\ref{eq:csd:smt-constraint-feature-cardinalty}) does not need to be specifically adapted for non-binary features in MaxSAT.
The same goes for the definition of alternative subgroup descriptions (cf.~Equations~\ref{eq:csd:smt-hamming} and Equation~\ref{eq:csd:smt-constraint-dissimilarity}).

\subsection{Proofs}
\label{sec:csd:appendix:proofs}

In this section, we provide proofs for propositions from Section~\ref{sec:csd:approach}.
Section~\ref{sec:csd:appendix:proofs:cardinality} proves complexity results for subgroup discovery with feature-cardinality constraints, while Section~\ref{sec:csd:appendix:proofs:alternatives} proves complexity results for searching alternative subgroup descriptions.

\subsubsection{Feature-Cardinality Constraints}
\label{sec:csd:appendix:proofs:cardinality}

\paragraph{Proof of Proposition~\ref{prop:csd:complexity-cardinality-perfect-subgroup}}
%
\begin{proof}
\end{proof}

\paragraph{Proof of Proposition~\ref{prop:csd:complexity-cardinality-imperfect-subgroup}}
%
\begin{proof}
\end{proof}

\subsubsection{Feature-Cardinality Constraints}
\label{sec:csd:appendix:proofs:alternatives}

\paragraph{Proof of Proposition~\ref{prop:csd:complexity-alternatives-perfect-subgroup}}
%
\begin{proof}
\end{proof}

\paragraph{Proof of Proposition~\ref{prop:csd:complexity-alternatives-imperfect-subgroup}}
%
\begin{proof}
\end{proof}

\renewcommand*{\bibfont}{\small} % use a smaller font for bib than for main text
\printbibliography

\end{document}
